{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MBA FIAP Inteligência Artificial & Machine Learning\r\n",
    "\r\n",
    "## Visão Computacional: Análise de Imagens Médicas\r\n",
    "\r\n",
    "## 1. Introdução\r\n",
    "\r\n",
    "As tecnologias de imagens médicas estão cada vez mais integradas aos sitemas de visão computacional, incluindo as imagens de raio-x.\r\n",
    "\r\n",
    "Modelos de equipamentos modernos geram imagens digitais deste tipo de exame, proporcionando análises mais completas e menos _ad-hoc_, com isso algumas pré-análises podem ser realizadas por aplicações baseadas em inteligência artificial para confirmar ou sugerir diagnósticos ao profissional responsável pelo exame.\r\n",
    "\r\n",
    "No campo dos diagósticos por raios-x, a pnenumonia é uma das enfermidades onde seu uso é um dos mais aplicados para determinar o curso de tratamento.\r\n",
    "\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/NORMAL2-IM-1422-0001.jpeg\">\r\n",
    "</p>\r\n",
    "\r\n",
    "## 2. Instruções\r\n",
    "\r\n",
    "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\r\n",
    "\r\n",
    "Por meio de uma trilha guiada, iremos constuir um modelo que seja capaz de classificar imagens de raio-x para determinar se a determinada pessoa está com alguma condição que necessita maiores cuidados.\r\n",
    "\r\n",
    "De acordo com as imagens disponíveis para o treinamento e validação, será de critério do grupo selecionar as quantidades ideais ou até mesmo pré-processar as imagens para obter o melhor resultado, nos principais indicadores de performance, como precisão, sensibilidade e pontuação F1.\r\n",
    "\r\n",
    "Este projeto poderá ser feita por grupos de até 4 pessoas.\r\n",
    "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\r\n",
    "\r\n",
    "| Nome dos Integrantes     | RM            | Turma |\r\n",
    "| :----------------------- | :------------- | :-----: |\r\n",
    "| Camila Okamura           | RM 339624      | `17IA` |\r\n",
    "| Cleiton Lima             | RM 339656      | `17IA` |\r\n",
    "| Henrique Faria           | RM 340214      | `17IA` |\r\n",
    "\r\n",
    "Por ser um projeto guiado, fique atento quando houver as marcações **Implementação** indica que é necessário realizar alguma implementação em Python no bloco a seguir onde há a inscrição ```##IMPLEMENTAR``` e **Resposta** indica que é esperado uma resposta objetiva relacionado a algum questionamento. \r\n",
    "\r\n",
    "**Cada grupo pode utilizar nas respostas objetivas quaisquer itens necessários que enriqueçam seu ponto vista, como gráficos, fotos e, até mesmo, trechos de código-fonte.**\r\n",
    "\r\n",
    "Pode-se utilizar quantos blocos forem necessários para realizar determinadas implementações ou utilizá-las para justificar as respostas. Não é obrigatório utilizar somente o bloco indicado.\r\n",
    "\r\n",
    "Ao final não se esqueça de subir os arquivos do projeto nas contas do GitHub de cada membro, ou subir na do representante do grupo e os membros realizarem o fork do projeto.\r\n",
    "\r\n",
    "A avaliação terá mais ênfase nos seguintes tópicos de desenvolvimento do projeto:\r\n",
    " \r\n",
    "1. __Pré-Processamento__\r\n",
    "2. __Classificação__\r\n",
    "3. __Performance__\r\n",
    "4. __Conclusões Finais__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Detalhe do problema: a pneunomia\r\n",
    "\r\n",
    "Fonte: [artigo](https://drauziovarella.uol.com.br/doencas-e-sintomas/pneumonia) do Dr. Drauzio Varella.\r\n",
    "\r\n",
    "Pneumonias são infecções que se instalam nos pulmões, órgãos duplos localizados um de cada lado da caixa torácica. Podem acometer a região dos alvéolos pulmonares onde desembocam as ramificações terminais dos brônquios e, às vezes, os interstícios (espaço entre um alvéolo e outro).\r\n",
    "\r\n",
    "Basicamente, pneumonia é provocada pela penetração de um agente infeccioso ou irritante (bactérias, vírus, fungos e por reações alérgicas) no espaço alveolar, onde ocorre a troca gasosa. Esse local deve estar sempre muito limpo, livre de substâncias que possam impedir o contato do ar com o sangue.\r\n",
    "\r\n",
    "Exame clínico, auscultação dos pulmões e radiografias de tórax são recursos essenciais para o diagnóstico de pneumonia.\r\n",
    "\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/pneumonia.jpeg\">\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Diagnóstico por raio-x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O exame de raio-x traz diferenças em cada tipo de diagnóstico, sendo considerado os seguintes grupos de análise: **normal** (ou controle) onde não há nenhuma condição de infeção, **bacterial pneumonia** (pneumonia bacteriana) que representa a condição de infecção bacteriana e **viral pneumonia** que indica a condição de infecção vira. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\r\n",
    "<img src=\"imagens/raiox.png\" height=\"60%\" width=\"60%\">\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As imagens de controle não são mais brancas ao centro que é onde fica o coração. Já nas imagens com pneumonia é possível notar regiões brancas ao redor dos pulmões, que é como o exame identifica as secreções responsáveis pela infeçcão.\n",
    "\n",
    "Quando mais regiões brancas ao redor do pulmão mais severa é a inflamação e menos se observa dos detalhes dos pulmões, ficando um pouco esmaecido diante desta condição."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Problema\n",
    "\n",
    "Construir um classificador utilizando _transfer learning_ para identificar as seguintes classes: **controle**, **pneumonia bacteriana** e **pneumonia viral**.\n",
    "\n",
    "Para construir este classificador, utilize o dataset do [Kaggle Chest Ray Pneumonia](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) e organize os dados de forma a separar em cada uma das classes que já estão definidas no diretório ```raiox```, sendo ```controle``` para as imagens normais (sem inflamação), ```bacteria``` para as imagens de pneumonia bacteriana e ```viral``` para as imagens de pneumonia viral.\n",
    "\n",
    "Determine a quantidade de imagens a serem treinadas e validadas. Utiliza pelo menos, 100 imagens para cada classe.\n",
    "\n",
    "Compare os resultados com pelo menos 3 classificadores, obtendo os valores de **precisão (precision)**, **sensibilidade (recall)** e **pontuação F1 (F1 Score)**. No guia abaixo, foi indicado os seguintes modelos: ResNet50, VGG16 e VGG19. \n",
    "\n",
    ">Importante: a escolha do número de imagens devem ser o suficiente para alcançar o valor de **precisão** mínima de 70%.\n",
    "\n",
    "A construção do modelo será utilizada o framework Keras."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Qual o número de imagens que foram selecionadas para cada classe?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:\r\n",
    "\r\n",
    "controle\r\n",
    "bacteria\r\n",
    "viral"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Componentes obrigatórios\n",
    "\n",
    "Este projeto requer a instalação dos seguintes componentes, via ```conda install```:\n",
    "\n",
    "* Keras\n",
    "* Tensorflow\n",
    "* Pillow\n",
    "* Matplotlib"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import shutil\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import tensorflow as tf\r\n",
    "import glob\r\n",
    "import cv2\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "import keras\r\n",
    "from keras.preprocessing.image import ImageDataGenerator\r\n",
    "from keras.applications import ResNet50, vgg16, vgg19, Xception, InceptionResNetV2\r\n",
    "from keras.applications import Xception\r\n",
    "from keras.applications.xception import preprocess_input\r\n",
    "from keras.applications.resnet import preprocess_input as res_pre_input\r\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre_input\r\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre_input\r\n",
    "from keras.applications.xception import preprocess_input as xception_pre_input\r\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input as inception_pre_input\r\n",
    "from keras import Model, layers, Sequential\r\n",
    "from keras.models import load_model, model_from_json\r\n",
    "\r\n",
    "import keras.backend as K\r\n",
    "\r\n",
    "plt.style.use('seaborn')\r\n",
    "sns.set_style(\"whitegrid\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Funções Auxiliares"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "def plot_history(history):\r\n",
    "    precision = history.history['precision_score']\r\n",
    "    val_precision = history.history['val_precision_score']\r\n",
    "    recall = history.history['recall_score']\r\n",
    "    val_recall = history.history['val_recall_score']\r\n",
    "    f1_score = history.history['f1_score']\r\n",
    "    val_f1_score = history.history['val_f1_score']\r\n",
    "    loss = history.history['loss']\r\n",
    "    val_loss = history.history['val_loss']\r\n",
    "    x = range(1, len(f1_score) + 1)\r\n",
    "    plt.figure(figsize=(20, 14))\r\n",
    "    plt.subplot(2, 2, 1)\r\n",
    "    plt.plot(x, precision, 'b', label='Training Precision')\r\n",
    "    plt.plot(x, val_precision, 'g', label='Validation Precision')\r\n",
    "    plt.title('Training and Validation Precision')\r\n",
    "    plt.legend()\r\n",
    "    plt.subplot(2, 2, 2)\r\n",
    "    plt.plot(x, recall, 'b', label='Training Recall')\r\n",
    "    plt.plot(x, val_recall, 'g', label='Validation Recall')\r\n",
    "    plt.title('Training and Validation Recall')\r\n",
    "    plt.legend()\r\n",
    "    plt.subplot(2, 2, 3)\r\n",
    "    plt.plot(x, f1_score, 'b', label='Training F1 Score')\r\n",
    "    plt.plot(x, val_f1_score, 'g', label='Validation F1 Score')\r\n",
    "    plt.title('Training and Validation F1 Score')\r\n",
    "    plt.legend()\r\n",
    "    plt.subplot(2, 2, 4)\r\n",
    "    plt.plot(x, loss, 'b', label='Training Loss')\r\n",
    "    plt.plot(x, val_loss, 'g', label='Validation Loss')\r\n",
    "    plt.title('Training and Validation Loss')\r\n",
    "    plt.legend()\r\n",
    "    \r\n",
    "class ModelCheckpoint_tweaked(tf.keras.callbacks.ModelCheckpoint):\r\n",
    "    def __init__(self,\r\n",
    "                   filepath,\r\n",
    "                   monitor='val_loss',\r\n",
    "                   verbose=0,\r\n",
    "                   save_best_only=False,\r\n",
    "                   save_weights_only=False,\r\n",
    "                   mode='auto',\r\n",
    "                   save_freq='epoch',\r\n",
    "                   options=None,\r\n",
    "                   **kwargs):\r\n",
    "        \r\n",
    "        #Change tf_utils source package.\r\n",
    "        from tensorflow.python.keras.utils import tf_utils\r\n",
    "        \r\n",
    "        super(ModelCheckpoint_tweaked, self).__init__(filepath,\r\n",
    "                   monitor,\r\n",
    "                   verbose,\r\n",
    "                   save_best_only,\r\n",
    "                   save_weights_only,\r\n",
    "                   mode,\r\n",
    "                   save_freq,\r\n",
    "                   options,\r\n",
    "                   **kwargs)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Carregando imagens de treinamento e validação\n",
    "\n",
    "Selecione a melhor divisão entre dados de treinamento e validação. O número deverá ser representado em número fracionário, 5% equivale a 0.05, por exemplo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analisando a base de dados original encontramos 3 pastas: train, test and val. A pasta val, de validação tem apenas 8 imagens normais e 8 imagens com pneumonia. Por ser muito pouco decidimos juntá-las a pasta de treino.<BR>Depois separamos os dados de treino e teste em 3 pastas, uma para cada uma das nossas classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_path = \"./raiox/train/\"\r\n",
    "test_path = \"./raiox/test/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotamos a distribuição das classses de treino e percebemos que temos uma quantidade muito maior da classe bactéria o que pode enviesar o nosso estudo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "normal = glob.glob(train_path + \"normal/*.jpeg\")\r\n",
    "virus = glob.glob(train_path + \"virus/*.jpeg\")\r\n",
    "bacteria = glob.glob(train_path + \"bacteria/*.jpeg\")\r\n",
    "\r\n",
    "sns.barplot(x = [\"normal\", \"virus\", \"bacteria\"], y = [len(normal), len(virus), len(bacteria)])\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"329.525312pt\" version=\"1.1\" viewBox=\"0 0 490.04375 329.525312\" width=\"490.04375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-27T18:42:57.818336</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 329.525312 \r\nL 490.04375 329.525312 \r\nL 490.04375 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.44375 306.18 \r\nL 482.84375 306.18 \r\nL 482.84375 7.2 \r\nL 36.44375 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"text_1\">\r\n      <!-- normal -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(95.561719 320.337812)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 422 0 \r\nL 422 3319 \r\nL 928 3319 \r\nL 928 2847 \r\nQ 1294 3394 1984 3394 \r\nQ 2284 3394 2536 3286 \r\nQ 2788 3178 2913 3003 \r\nQ 3038 2828 3088 2588 \r\nQ 3119 2431 3119 2041 \r\nL 3119 0 \r\nL 2556 0 \r\nL 2556 2019 \r\nQ 2556 2363 2490 2533 \r\nQ 2425 2703 2258 2804 \r\nQ 2091 2906 1866 2906 \r\nQ 1506 2906 1245 2678 \r\nQ 984 2450 984 1813 \r\nL 984 0 \r\nL 422 0 \r\nz\r\n\" id=\"ArialMT-6e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 213 1659 \r\nQ 213 2581 725 3025 \r\nQ 1153 3394 1769 3394 \r\nQ 2453 3394 2887 2945 \r\nQ 3322 2497 3322 1706 \r\nQ 3322 1066 3130 698 \r\nQ 2938 331 2570 128 \r\nQ 2203 -75 1769 -75 \r\nQ 1072 -75 642 372 \r\nQ 213 819 213 1659 \r\nz\r\nM 791 1659 \r\nQ 791 1022 1069 705 \r\nQ 1347 388 1769 388 \r\nQ 2188 388 2466 706 \r\nQ 2744 1025 2744 1678 \r\nQ 2744 2294 2464 2611 \r\nQ 2184 2928 1769 2928 \r\nQ 1347 2928 1069 2612 \r\nQ 791 2297 791 1659 \r\nz\r\n\" id=\"ArialMT-6f\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 416 0 \r\nL 416 3319 \r\nL 922 3319 \r\nL 922 2816 \r\nQ 1116 3169 1280 3281 \r\nQ 1444 3394 1641 3394 \r\nQ 1925 3394 2219 3213 \r\nL 2025 2691 \r\nQ 1819 2813 1613 2813 \r\nQ 1428 2813 1281 2702 \r\nQ 1134 2591 1072 2394 \r\nQ 978 2094 978 1738 \r\nL 978 0 \r\nL 416 0 \r\nz\r\n\" id=\"ArialMT-72\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 422 0 \r\nL 422 3319 \r\nL 925 3319 \r\nL 925 2853 \r\nQ 1081 3097 1340 3245 \r\nQ 1600 3394 1931 3394 \r\nQ 2300 3394 2536 3241 \r\nQ 2772 3088 2869 2813 \r\nQ 3263 3394 3894 3394 \r\nQ 4388 3394 4653 3120 \r\nQ 4919 2847 4919 2278 \r\nL 4919 0 \r\nL 4359 0 \r\nL 4359 2091 \r\nQ 4359 2428 4304 2576 \r\nQ 4250 2725 4106 2815 \r\nQ 3963 2906 3769 2906 \r\nQ 3419 2906 3187 2673 \r\nQ 2956 2441 2956 1928 \r\nL 2956 0 \r\nL 2394 0 \r\nL 2394 2156 \r\nQ 2394 2531 2256 2718 \r\nQ 2119 2906 1806 2906 \r\nQ 1569 2906 1367 2781 \r\nQ 1166 2656 1075 2415 \r\nQ 984 2175 984 1722 \r\nL 984 0 \r\nL 422 0 \r\nz\r\n\" id=\"ArialMT-6d\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2588 409 \r\nQ 2275 144 1986 34 \r\nQ 1697 -75 1366 -75 \r\nQ 819 -75 525 192 \r\nQ 231 459 231 875 \r\nQ 231 1119 342 1320 \r\nQ 453 1522 633 1644 \r\nQ 813 1766 1038 1828 \r\nQ 1203 1872 1538 1913 \r\nQ 2219 1994 2541 2106 \r\nQ 2544 2222 2544 2253 \r\nQ 2544 2597 2384 2738 \r\nQ 2169 2928 1744 2928 \r\nQ 1347 2928 1158 2789 \r\nQ 969 2650 878 2297 \r\nL 328 2372 \r\nQ 403 2725 575 2942 \r\nQ 747 3159 1072 3276 \r\nQ 1397 3394 1825 3394 \r\nQ 2250 3394 2515 3294 \r\nQ 2781 3194 2906 3042 \r\nQ 3031 2891 3081 2659 \r\nQ 3109 2516 3109 2141 \r\nL 3109 1391 \r\nQ 3109 606 3145 398 \r\nQ 3181 191 3288 0 \r\nL 2700 0 \r\nQ 2613 175 2588 409 \r\nz\r\nM 2541 1666 \r\nQ 2234 1541 1622 1453 \r\nQ 1275 1403 1131 1340 \r\nQ 988 1278 909 1158 \r\nQ 831 1038 831 891 \r\nQ 831 666 1001 516 \r\nQ 1172 366 1500 366 \r\nQ 1825 366 2078 508 \r\nQ 2331 650 2450 897 \r\nQ 2541 1088 2541 1459 \r\nL 2541 1666 \r\nz\r\n\" id=\"ArialMT-61\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 409 0 \r\nL 409 4581 \r\nL 972 4581 \r\nL 972 0 \r\nL 409 0 \r\nz\r\n\" id=\"ArialMT-6c\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-6e\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-6f\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-72\"/>\r\n       <use x=\"144.53125\" xlink:href=\"#ArialMT-6d\"/>\r\n       <use x=\"227.832031\" xlink:href=\"#ArialMT-61\"/>\r\n       <use x=\"283.447266\" xlink:href=\"#ArialMT-6c\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"text_2\">\r\n      <!-- virus -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(249.0875 320.337812)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1344 0 \r\nL 81 3319 \r\nL 675 3319 \r\nL 1388 1331 \r\nQ 1503 1009 1600 663 \r\nQ 1675 925 1809 1294 \r\nL 2547 3319 \r\nL 3125 3319 \r\nL 1869 0 \r\nL 1344 0 \r\nz\r\n\" id=\"ArialMT-76\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 425 3934 \r\nL 425 4581 \r\nL 988 4581 \r\nL 988 3934 \r\nL 425 3934 \r\nz\r\nM 425 0 \r\nL 425 3319 \r\nL 988 3319 \r\nL 988 0 \r\nL 425 0 \r\nz\r\n\" id=\"ArialMT-69\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2597 0 \r\nL 2597 488 \r\nQ 2209 -75 1544 -75 \r\nQ 1250 -75 995 37 \r\nQ 741 150 617 320 \r\nQ 494 491 444 738 \r\nQ 409 903 409 1263 \r\nL 409 3319 \r\nL 972 3319 \r\nL 972 1478 \r\nQ 972 1038 1006 884 \r\nQ 1059 663 1231 536 \r\nQ 1403 409 1656 409 \r\nQ 1909 409 2131 539 \r\nQ 2353 669 2445 892 \r\nQ 2538 1116 2538 1541 \r\nL 2538 3319 \r\nL 3100 3319 \r\nL 3100 0 \r\nL 2597 0 \r\nz\r\n\" id=\"ArialMT-75\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 197 991 \r\nL 753 1078 \r\nQ 800 744 1014 566 \r\nQ 1228 388 1613 388 \r\nQ 2000 388 2187 545 \r\nQ 2375 703 2375 916 \r\nQ 2375 1106 2209 1216 \r\nQ 2094 1291 1634 1406 \r\nQ 1016 1563 777 1677 \r\nQ 538 1791 414 1992 \r\nQ 291 2194 291 2438 \r\nQ 291 2659 392 2848 \r\nQ 494 3038 669 3163 \r\nQ 800 3259 1026 3326 \r\nQ 1253 3394 1513 3394 \r\nQ 1903 3394 2198 3281 \r\nQ 2494 3169 2634 2976 \r\nQ 2775 2784 2828 2463 \r\nL 2278 2388 \r\nQ 2241 2644 2061 2787 \r\nQ 1881 2931 1553 2931 \r\nQ 1166 2931 1000 2803 \r\nQ 834 2675 834 2503 \r\nQ 834 2394 903 2306 \r\nQ 972 2216 1119 2156 \r\nQ 1203 2125 1616 2013 \r\nQ 2213 1853 2448 1751 \r\nQ 2684 1650 2818 1456 \r\nQ 2953 1263 2953 975 \r\nQ 2953 694 2789 445 \r\nQ 2625 197 2315 61 \r\nQ 2006 -75 1616 -75 \r\nQ 969 -75 630 194 \r\nQ 291 463 197 991 \r\nz\r\n\" id=\"ArialMT-73\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-76\"/>\r\n       <use x=\"50\" xlink:href=\"#ArialMT-69\"/>\r\n       <use x=\"72.216797\" xlink:href=\"#ArialMT-72\"/>\r\n       <use x=\"105.517578\" xlink:href=\"#ArialMT-75\"/>\r\n       <use x=\"161.132812\" xlink:href=\"#ArialMT-73\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"text_3\">\r\n      <!-- bacteria -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(390.657031 320.337812)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 941 0 \r\nL 419 0 \r\nL 419 4581 \r\nL 981 4581 \r\nL 981 2947 \r\nQ 1338 3394 1891 3394 \r\nQ 2197 3394 2470 3270 \r\nQ 2744 3147 2920 2923 \r\nQ 3097 2700 3197 2384 \r\nQ 3297 2069 3297 1709 \r\nQ 3297 856 2875 390 \r\nQ 2453 -75 1863 -75 \r\nQ 1275 -75 941 416 \r\nL 941 0 \r\nz\r\nM 934 1684 \r\nQ 934 1088 1097 822 \r\nQ 1363 388 1816 388 \r\nQ 2184 388 2453 708 \r\nQ 2722 1028 2722 1663 \r\nQ 2722 2313 2464 2622 \r\nQ 2206 2931 1841 2931 \r\nQ 1472 2931 1203 2611 \r\nQ 934 2291 934 1684 \r\nz\r\n\" id=\"ArialMT-62\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2588 1216 \r\nL 3141 1144 \r\nQ 3050 572 2676 248 \r\nQ 2303 -75 1759 -75 \r\nQ 1078 -75 664 370 \r\nQ 250 816 250 1647 \r\nQ 250 2184 428 2587 \r\nQ 606 2991 970 3192 \r\nQ 1334 3394 1763 3394 \r\nQ 2303 3394 2647 3120 \r\nQ 2991 2847 3088 2344 \r\nL 2541 2259 \r\nQ 2463 2594 2264 2762 \r\nQ 2066 2931 1784 2931 \r\nQ 1359 2931 1093 2626 \r\nQ 828 2322 828 1663 \r\nQ 828 994 1084 691 \r\nQ 1341 388 1753 388 \r\nQ 2084 388 2306 591 \r\nQ 2528 794 2588 1216 \r\nz\r\n\" id=\"ArialMT-63\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 1650 503 \r\nL 1731 6 \r\nQ 1494 -44 1306 -44 \r\nQ 1000 -44 831 53 \r\nQ 663 150 594 308 \r\nQ 525 466 525 972 \r\nL 525 2881 \r\nL 113 2881 \r\nL 113 3319 \r\nL 525 3319 \r\nL 525 4141 \r\nL 1084 4478 \r\nL 1084 3319 \r\nL 1650 3319 \r\nL 1650 2881 \r\nL 1084 2881 \r\nL 1084 941 \r\nQ 1084 700 1114 631 \r\nQ 1144 563 1211 522 \r\nQ 1278 481 1403 481 \r\nQ 1497 481 1650 503 \r\nz\r\n\" id=\"ArialMT-74\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2694 1069 \r\nL 3275 997 \r\nQ 3138 488 2766 206 \r\nQ 2394 -75 1816 -75 \r\nQ 1088 -75 661 373 \r\nQ 234 822 234 1631 \r\nQ 234 2469 665 2931 \r\nQ 1097 3394 1784 3394 \r\nQ 2450 3394 2872 2941 \r\nQ 3294 2488 3294 1666 \r\nQ 3294 1616 3291 1516 \r\nL 816 1516 \r\nQ 847 969 1125 678 \r\nQ 1403 388 1819 388 \r\nQ 2128 388 2347 550 \r\nQ 2566 713 2694 1069 \r\nz\r\nM 847 1978 \r\nL 2700 1978 \r\nQ 2663 2397 2488 2606 \r\nQ 2219 2931 1791 2931 \r\nQ 1403 2931 1139 2672 \r\nQ 875 2413 847 1978 \r\nz\r\n\" id=\"ArialMT-65\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-62\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-61\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-63\"/>\r\n       <use x=\"161.230469\" xlink:href=\"#ArialMT-74\"/>\r\n       <use x=\"189.013672\" xlink:href=\"#ArialMT-65\"/>\r\n       <use x=\"244.628906\" xlink:href=\"#ArialMT-72\"/>\r\n       <use x=\"277.929688\" xlink:href=\"#ArialMT-69\"/>\r\n       <use x=\"300.146484\" xlink:href=\"#ArialMT-61\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 306.18 \r\nL 482.84375 306.18 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(23.882813 309.758906)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 266 2259 \r\nQ 266 3072 433 3567 \r\nQ 600 4063 929 4331 \r\nQ 1259 4600 1759 4600 \r\nQ 2128 4600 2406 4451 \r\nQ 2684 4303 2865 4023 \r\nQ 3047 3744 3150 3342 \r\nQ 3253 2941 3253 2259 \r\nQ 3253 1453 3087 958 \r\nQ 2922 463 2592 192 \r\nQ 2263 -78 1759 -78 \r\nQ 1097 -78 719 397 \r\nQ 266 969 266 2259 \r\nz\r\nM 844 2259 \r\nQ 844 1131 1108 757 \r\nQ 1372 384 1759 384 \r\nQ 2147 384 2411 759 \r\nQ 2675 1134 2675 2259 \r\nQ 2675 3391 2411 3762 \r\nQ 2147 4134 1753 4134 \r\nQ 1366 4134 1134 3806 \r\nQ 844 3388 844 2259 \r\nz\r\n\" id=\"ArialMT-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_2\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 250.084086 \r\nL 482.84375 250.084086 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 500 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(12.760938 253.662993)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 266 1200 \r\nL 856 1250 \r\nQ 922 819 1161 601 \r\nQ 1400 384 1738 384 \r\nQ 2144 384 2425 690 \r\nQ 2706 997 2706 1503 \r\nQ 2706 1984 2436 2262 \r\nQ 2166 2541 1728 2541 \r\nQ 1456 2541 1237 2417 \r\nQ 1019 2294 894 2097 \r\nL 366 2166 \r\nL 809 4519 \r\nL 3088 4519 \r\nL 3088 3981 \r\nL 1259 3981 \r\nL 1013 2750 \r\nQ 1425 3038 1878 3038 \r\nQ 2478 3038 2890 2622 \r\nQ 3303 2206 3303 1553 \r\nQ 3303 931 2941 478 \r\nQ 2500 -78 1738 -78 \r\nQ 1113 -78 717 272 \r\nQ 322 622 266 1200 \r\nz\r\n\" id=\"ArialMT-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-35\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 193.988173 \r\nL 482.84375 193.988173 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 197.567079)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2384 0 \r\nL 1822 0 \r\nL 1822 3584 \r\nQ 1619 3391 1289 3197 \r\nQ 959 3003 697 2906 \r\nL 697 3450 \r\nQ 1169 3672 1522 3987 \r\nQ 1875 4303 2022 4600 \r\nL 2384 4600 \r\nL 2384 0 \r\nz\r\n\" id=\"ArialMT-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-31\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_4\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 137.892259 \r\nL 482.84375 137.892259 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1500 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 141.471166)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#ArialMT-31\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 81.796346 \r\nL 482.84375 81.796346 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2000 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 85.375252)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 3222 541 \r\nL 3222 0 \r\nL 194 0 \r\nQ 188 203 259 391 \r\nQ 375 700 629 1000 \r\nQ 884 1300 1366 1694 \r\nQ 2113 2306 2375 2664 \r\nQ 2638 3022 2638 3341 \r\nQ 2638 3675 2398 3904 \r\nQ 2159 4134 1775 4134 \r\nQ 1369 4134 1125 3890 \r\nQ 881 3647 878 3216 \r\nL 300 3275 \r\nQ 359 3922 746 4261 \r\nQ 1134 4600 1788 4600 \r\nQ 2447 4600 2831 4234 \r\nQ 3216 3869 3216 3328 \r\nQ 3216 3053 3103 2787 \r\nQ 2991 2522 2730 2228 \r\nQ 2469 1934 1863 1422 \r\nQ 1356 997 1212 845 \r\nQ 1069 694 975 541 \r\nL 3222 541 \r\nz\r\n\" id=\"ArialMT-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#ArialMT-32\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_6\">\r\n      <path clip-path=\"url(#p5e720640cd)\" d=\"M 36.44375 25.700432 \r\nL 482.84375 25.700432 \r\n\" style=\"fill:none;stroke:#cccccc;stroke-linecap:round;\"/>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2500 -->\r\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 29.279339)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#ArialMT-32\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-35\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-30\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p5e720640cd)\" d=\"M 51.32375 306.18 \r\nL 170.36375 306.18 \r\nL 170.36375 154.833225 \r\nL 51.32375 154.833225 \r\nz\r\n\" style=\"fill:#5875a4;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.3;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p5e720640cd)\" d=\"M 200.12375 306.18 \r\nL 319.16375 306.18 \r\nL 319.16375 155.281993 \r\nL 200.12375 155.281993 \r\nz\r\n\" style=\"fill:#5f9e6e;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.3;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p5e720640cd)\" d=\"M 348.92375 306.18 \r\nL 467.96375 306.18 \r\nL 467.96375 21.437143 \r\nL 348.92375 21.437143 \r\nz\r\n\" style=\"fill:#b55d60;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.3;\"/>\r\n   </g>\r\n   <g id=\"line2d_7\">\r\n    <path clip-path=\"url(#p5e720640cd)\" style=\"fill:none;stroke:#424242;stroke-linecap:round;stroke-width:3.15;\"/>\r\n   </g>\r\n   <g id=\"line2d_8\">\r\n    <path clip-path=\"url(#p5e720640cd)\" style=\"fill:none;stroke:#424242;stroke-linecap:round;stroke-width:3.15;\"/>\r\n   </g>\r\n   <g id=\"line2d_9\">\r\n    <path clip-path=\"url(#p5e720640cd)\" style=\"fill:none;stroke:#424242;stroke-linecap:round;stroke-width:3.15;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.44375 306.18 \r\nL 36.44375 7.2 \r\n\" style=\"fill:none;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 482.84375 306.18 \r\nL 482.84375 7.2 \r\n\" style=\"fill:none;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 36.44375 306.18 \r\nL 482.84375 306.18 \r\n\" style=\"fill:none;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 36.44375 7.2 \r\nL 482.84375 7.2 \r\n\" style=\"fill:none;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p5e720640cd\">\r\n   <rect height=\"298.98\" width=\"446.4\" x=\"36.44375\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vamos retirar as imagens da classe bacteria que possuem uma proporção entre X e Y menor que 0.67 para equalizarmos a quantidade das três classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "bacteria_shape = []\r\n",
    "for idx, file in enumerate(bacteria):\r\n",
    "    imagem = cv2.imread(file)\r\n",
    "    shape = imagem.shape[0]/imagem.shape[1]\r\n",
    "    if shape < 0.67:\r\n",
    "        bacteria_shape.append(idx)\r\n",
    "print(len(bacteria_shape))\r\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Rodar apenas na primeira vez para separar as imagens\r\n",
    "for idx in bacteria_shape:\r\n",
    "    shutil.move(bacteria[idx], train_path + \"../bacteria_retirados/\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bacteria = glob.glob(train_path + \"bacteria/*.jpeg\")\r\n",
    "sns.barplot(x = [\"normal\", \"virus\", \"bacteria\"], y = [len(normal), len(virus), len(bacteria)])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normal = glob.glob(test_path + \"normal/*.jpeg\")\r\n",
    "virus = glob.glob(test_path + \"virus/*.jpeg\")\r\n",
    "bacteria = glob.glob(test_path + \"bacteria/*.jpeg\")\r\n",
    "\r\n",
    "sns.barplot(x = [\"normal\", \"virus\", \"bacteria\"], y = [len(normal), len(virus), len(bacteria)])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "divisao_treino_validacao = 0.3\r\n",
    "seed_treino_validacao = 7;"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "train_datagen = ImageDataGenerator(\r\n",
    "    validation_split=divisao_treino_validacao,\r\n",
    "    rotation_range=0.1,\r\n",
    "    preprocessing_function=res_pre_input)\r\n",
    "\r\n",
    "train_generator = train_datagen.flow_from_directory(\r\n",
    "    \"raiox/train/\",\r\n",
    "    seed=seed_treino_validacao,\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(224,224),\r\n",
    "    subset=\"training\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#val_datagen = ImageDataGenerator(preprocessing_function=res_pre_input)\r\n",
    "\r\n",
    "val_generator = train_datagen.flow_from_directory(\r\n",
    "    \"raiox/train/\",\r\n",
    "    seed=seed_treino_validacao,\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(224,224),\r\n",
    "    subset=\"validation\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=res_pre_input)\r\n",
    "\r\n",
    "test_generator = test_datagen.flow_from_directory(\r\n",
    "    \"raiox/test/\",\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(224,224))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"classes do conjunto de treino: {train_generator.class_indices}\")\r\n",
    "print(f\"classes do conjunto de validação: {val_generator.class_indices}\")\r\n",
    "print(f\"classes do conjunto de teste: {test_generator.class_indices}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 Modelos de transfer learning\n",
    "\n",
    "O Keras já possui classes especializadas para os seguintes modelos de deep-learning treinados com o conjunto de dados [ImageNet](http://www.image-net.org/):\n",
    "  \n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "* DenseNet\n",
    "* NASNet\n",
    "* MobileNetV2\n",
    "\n",
    "Mais detalhes, veja na [documentação do Keras](https://keras.io/applications/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para este estudo, vamos utilizar para avaliação as seguintes arquiteturas: RestNet50, VGG16 e VGG19."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Indicadores de desempenho\n",
    "\n",
    "O Keras não possui os indicadores de desempenho como precisão, sensibilidade e pontuação f1 por padrão, portanto precisamos implementar externamente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def recall_score(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
    "    return recall\r\n",
    "\r\n",
    "def precision_score(y_true, y_pred):\r\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
    "    return precision\r\n",
    "\r\n",
    "def f1_score(y_true, y_pred):\r\n",
    "    precision = precision_score(y_true, y_pred)\r\n",
    "    recall = recall_score(y_true, y_pred)\r\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5.1 Arquitetura ResNet50"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Explique como é constituída a arquitetura do ResNet50? *Utilize, se necessário, gráficos, projetos que utilizam essa arquitetura. Detalhe também sua topologia em camadas e mostre quais as situações essa arquitetura pode ter mais êxito e quais cenários não tem.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=\"center\">\r\n",
    "    <strong>DEEP RESIDUAL NETWORK   ResNet-50</strong>\r\n",
    "</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/ResNet50_architecture-1.png\" height=\"80%\" width=\"80%\">\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>A imagem acima representa todas as camadas de uma rede ResNet-50. A rede Resnet-50 é uma rede neural residual com 50 camadas de profundidade, por isso o nome terminado em 50. Existem outros modelos que seguem a mesma arquitetura mas com profundidades diferentes como: ResNet-18, ResNet-34, ResNet-101 e ResNet-152.\r\n",
    "</h3>\r\n",
    "<h3>\r\n",
    "Ela foi apresentada pela primeira vez na competição anual de 2015 da ImageNet e ganhou o primeiro lugar em diversas categorias. Desde então é muito utilizada em trabalhos de reconhecimento de imagens, mas também em outros campos da ciência e enegenharia.\r\n",
    "</h3>\r\n",
    "<h3>\r\n",
    "Sua configuração é de uma rede neural residual, ou seja, utiliza um bloco de código chamado de \"Bloco Residual\" que usa uma conexão entre a sua entrada e saida chamada de \"Identity Connection\".\r\n",
    "</h3>\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/Simple_Residual_Block.png\" height=\"40%\" width=\"40%\">\r\n",
    "</p>\r\n",
    "<h3>\r\n",
    "A idéia por trás deste modelo é reduzir o aprendizado ruim das camadas iniciais de uma rede neural profunda. O que se descobriu estudando modelos mais tradicionais de redes neurais profundas é que o processo de \"Backpropagation\" utilizado para atualizar os pesos da rede fuciona bem em redes rasas e nas camadas finais de uma rede profunda. Mas acaba não atingindo de forma efetiva as camadas iniciais da rede. Este efeito é chamada de \"Vanishing Gradient\".\r\n",
    "</h3>\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/vanishing_grad-1.png\" height=\"40%\" width=\"40%\">\r\n",
    "</p>\r\n",
    "<h3>\r\n",
    "Existem outras formas de minimizar este problema, mas para as redes neurais muito profundas estes métodos não estavam melhorando a acuracia do modelo. Para atacar este problema os idealizadores da ResNet-50 resolveram utilizar o bloco residual de forma a incrementar os valores de ajuste que serão aplicados aos pesos das camadas iniciais através da \"Identity Connection\".\r\n",
    "</h3>\r\n",
    "<p align=\"center\">\r\n",
    "    <img src=\"imagens/Grad_path.webp\" height=\"50%\" width=\"50%\">\r\n",
    "</p>\r\n",
    "<h3>\r\n",
    "Como vemos na imagem acima quando o modelo esta execuntando a \"Backpropagation\" para ajustar os pesos ele recebe a informação por duas vias, sendo que na da \"Identity\" ele não sofre a influência dos pesos de um determinado bloco de camadas, o que auxilia ao valor de ajuste a chegar nas camadas iniciais. Desta forma é esperado que uma rede neural profunda residual consiga ter uma melhor performance em diversas situações.\r\n",
    "<h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A técnica de transfer learning consiste de utilizar o mesmo modelo e treiná-lo para outas imagens. Por tal motivo, excluímos a última camada para modelar com as classes que definimos, ou seja, **controle**, **bacteriana** e **viral**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Informe a quantidade de classes a serem classificadas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## IMPLEMENTE\r\n",
    "\r\n",
    "qtde_classes = len(train_generator.class_indices)\r\n",
    "qtde_classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conv_base = ResNet50(include_top=False)\r\n",
    "\r\n",
    "for layer in conv_base.layers:\r\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = conv_base.output\r\n",
    "x = layers.GlobalAveragePooling2D()(x)\r\n",
    "x = layers.Dense(1024, activation='relu')(x)\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "\r\n",
    "predictions = layers.Dense(qtde_classes, activation='softmax')(x)\r\n",
    "model = Model(conv_base.input, predictions)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = keras.optimizers.Adam()\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[precision_score, recall_score, f1_score])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O número de épocas define quantas vezes o modelo irá treinar e validar o erro, assim ajustando os pesos para melhor convergência.\n",
    "Escolha o número adequado de épocas para alcançarmos pelo menos **70% de precisão de validação**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## IMPLEMENTE\r\n",
    "\r\n",
    "qtde_epocas = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpointer = ModelCheckpoint_tweaked(filepath='./modelo_resnet50.hdf5', verbose=1,  mode='max', save_best_only=True, monitor='val_f1_score')\r\n",
    "\r\n",
    "history = model.fit(train_generator, \r\n",
    "                    epochs=qtde_epocas,\r\n",
    "                    validation_steps=5,\r\n",
    "                    steps_per_epoch=5,\r\n",
    "                    validation_data=val_generator,\r\n",
    "                    callbacks=[checkpointer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um modelo que converge bem possui o gráfico de perda (*loss*) descendente e os gráfico de precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*) em acendente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Avalie os gráficos de perda (*loss*), precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*)  e explique o comportamento de ambos no que tange a convergência do modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Quais são os valores de **precisão (precision)**, **sensibilidade (recall)** de validação? \n",
    "\n",
    "*Estes valores são exibidos durante o treinamento, utilize a última saída, exemplo:*\n",
    "\n",
    "```\n",
    "Epoch 10/10 [==============================] - 45s 9s/step - loss: 0.1234 - precision_score: 0.9742 - recall_score: 0.9683 - f1_score: 0.9712 - val_loss: 0.8819 - val_precision_score: 0.6912 - val_recall_score: 0.5649 - val_f1_score: 0.6216```\n",
    "\n",
    "No caso acima, o valor de precisão, sensibilidade e pontuação de validação são, respectivamente, 69,12%, 56,49% e 62,16%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = load_model(\"./modelo_resnet50.hdf5\", custom_objects={'f1_score': f1_score, 'precision_score': precision_score, 'recall_score': recall_score})\r\n",
    "\r\n",
    "scores_resnet50 = model.evaluate(val_generator)\r\n",
    "print()\r\n",
    "print(f\"Precision Score: {round(scores_resnet50[1]*100,2)}%\")\r\n",
    "print(f\"Recall Score: {round(scores_resnet50[2]*100,2)}%\")\r\n",
    "print(f\"F1 Score: {round(scores_resnet50[3]*100,2)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5.2 Arquitetura VGG16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Explique como é constituída a arquitetura do VGG16? *Utilize, se necessário, gráficos, projetos que utilizam essa arquitetura. Detalhe também sua topologia em camadas e mostre quais as situações essa arquitetura pode ter mais êxito e quais cenários não tem.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conv_base = vgg16.VGG16(include_top=False) \r\n",
    "\r\n",
    "for layer in conv_base.layers:\r\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = conv_base.output\r\n",
    "x = layers.GlobalAveragePooling2D()(x)\r\n",
    "x = layers.Dense(1024, activation='relu')(x)\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "\r\n",
    "predictions = layers.Dense(qtde_classes, activation='softmax')(x)\r\n",
    "model = Model(conv_base.input, predictions)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = keras.optimizers.Adam()\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[precision_score, recall_score, f1_score])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpointer = ModelCheckpoint_tweaked(filepath='./modelo_vgg16.hdf5', verbose=1,  mode='max', save_best_only=True, monitor='val_f1_score')\r\n",
    "\r\n",
    "history = model.fit(train_generator, \r\n",
    "                    epochs=qtde_epocas, \r\n",
    "                    validation_steps=5, \r\n",
    "                    steps_per_epoch=5, \r\n",
    "                    validation_data=val_generator,\r\n",
    "                    callbacks=[checkpointer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um modelo que converge bem possui o gráfico de perda (*loss*) descendente e os gráfico de precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*) em acendente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Avalie os gráficos de perda (*loss*), precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*)  e explique o comportamento de ambos no que tange a convergência do modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Quais são os valores de **precisão (precision)**, **sensibilidade (recall)** de validação? \n",
    "\n",
    "*Estes valores são exibidos durante o treinamento, utilize a última saída, exemplo:*\n",
    "\n",
    "```\n",
    "Epoch 10/10 [==============================] - 45s 9s/step - loss: 0.1234 - precision_score: 0.9742 - recall_score: 0.9683 - f1_score: 0.9712 - val_loss: 0.8819 - val_precision_score: 0.6912 - val_recall_score: 0.5649 - val_f1_score: 0.6216```\n",
    "\n",
    "No caso acima, o valor de precisão, sensibilidade e pontuação de validação são, respectivamente, 69,12%, 56,49% e 62,16%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = load_model(\"./modelo_vgg16.hdf5\", custom_objects={'f1_score': f1_score, 'precision_score': precision_score, 'recall_score': recall_score})\r\n",
    "\r\n",
    "scores_vgg16 = model.evaluate(test_generator)\r\n",
    "print()\r\n",
    "print(f\"Precision Score: {round(scores_vgg16[1]*100,2)}%\")\r\n",
    "print(f\"Recall Score: {round(scores_vgg16[2]*100,2)}%\")\r\n",
    "print(f\"F1 Score: {round(scores_vgg16[3]*100,2)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5.3 Arquitetura VGG19"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Explique como é constituída a arquitetura do VGG19? *Utilize, se necessário, gráficos, projetos que utilizam essa arquitetura. Detalhe também sua topologia em camadas e mostre quais as situações essa arquitetura pode ter mais êxito e quais cenários não tem.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conv_base = vgg19.VGG19(include_top=False)\r\n",
    "\r\n",
    "for layer in conv_base.layers:\r\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = conv_base.output\r\n",
    "x = layers.GlobalAveragePooling2D()(x)\r\n",
    "x = layers.Dense(1024, activation='relu')(x) \r\n",
    "x = layers.Dropout(0.5)(x) \r\n",
    "\r\n",
    "predictions = layers.Dense(qtde_classes, activation='softmax')(x)\r\n",
    "model = Model(conv_base.input, predictions)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = keras.optimizers.Adam()\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[precision_score, recall_score, f1_score])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpointer = ModelCheckpoint_tweaked(filepath='./modelo_vgg19.hdf5', verbose=1,  mode='max', save_best_only=True, monitor='val_f1_score')\r\n",
    "\r\n",
    "history = model.fit_generator(generator=train_generator, \r\n",
    "                              epochs=qtde_epocas, \r\n",
    "                              validation_steps=5, \r\n",
    "                              steps_per_epoch=5, \r\n",
    "                              validation_data=val_generator,\r\n",
    "                              callbacks=[checkpointer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um modelo que converge bem possui o gráfico de perda (*loss*) descendente e os gráfico de precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*) em acendente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Avalie os gráficos de perda (*loss*), precisão (*precision*), sensibilidade (*recall*) e pontuação f1 (*f1 score*)  e explique o comportamento de ambos no que tange a convergência do modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pergunta**: Quais são os valores de **precisão (precision)**, **sensibilidade (recall)** de validação? \n",
    "\n",
    "*Estes valores são exibidos durante o treinamento, utilize a última saída, exemplo:*\n",
    "\n",
    "```\n",
    "Epoch 10/10 [==============================] - 45s 9s/step - loss: 0.1234 - precision_score: 0.9742 - recall_score: 0.9683 - f1_score: 0.9712 - val_loss: 0.8819 - val_precision_score: 0.6912 - val_recall_score: 0.5649 - val_f1_score: 0.6216```\n",
    "\n",
    "No caso acima, o valor de precisão, sensibilidade e pontuação de validação são, respectivamente, 69,12%, 56,49% e 62,16%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = load_model(\"./modelo_vgg19.hdf5\", custom_objects={'f1_score': f1_score, 'precision_score': precision_score, 'recall_score': recall_score})\r\n",
    "\r\n",
    "scores_vgg19 = model.evaluate(test_generator)\r\n",
    "print()\r\n",
    "print(f\"Precision Score: {round(scores_vgg19[1]*100,2)}%\")\r\n",
    "print(f\"Recall Score: {round(scores_vgg19[2]*100,2)}%\")\r\n",
    "print(f\"F1 Score: {round(scores_vgg19[3]*100,2)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 Compartivo de arquiteturas\n",
    "\n",
    "Preencha a tabela abaixo com os valores dos indicadores de performance apresentados.\n",
    "\n",
    "_O cálculo do F1-Score é dado por 2 * (Precisão * Sensibilidade) / (Precisão + Sensibilidade)._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Modelo\\t\\t| Precisão (*Precision*) | Sensibilidade (*Recall*) | F1-Score\")\r\n",
    "print()\r\n",
    "print(f\"ResNet50\\t| {round(scores_resnet50[1],6)*100} %\\t\\t | {round(scores_resnet50[2],6)*100} %\\t\\t | {round(scores_resnet50[3],6)*100} %\")\r\n",
    "print(f\"VGG16\\t\\t| {round(scores_vgg16[1],6)*100} %\\t\\t | {round(scores_vgg16[2],6)*100} %\\t\\t | {round(scores_vgg16[3],6)*100} %\")\r\n",
    "print(f\"VGG19\\t\\t| {round(scores_vgg19[1],6)*100} %\\t\\t | {round(scores_vgg19[2],6)*100} %\\t\\t | {round(scores_vgg19[3],6)*100} %\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.7 Conclusões\n",
    "\n",
    "Analise os resultados da tabela de indicadores do comparativo de arquiteturas e explique os principais motivos pelos quais cada modelo obteve cada resultado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Respota**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.8 Abordagem Extra\n",
    "\n",
    "Considerando os outros classificadores, escolha outro que ainda não foi utilizado, implemente abaixo. Ao final compare os resultados e explique os resultados.\n",
    "\n",
    "_Não se esquece de utilizar as importações adequadas para cada modelo.\n",
    "A forma de implementação deve respeitar as mesmas condições como valor de split e quantidade de imagens para poder comparar os modelos._"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "train_datagen = ImageDataGenerator(\r\n",
    "    validation_split=divisao_treino_validacao,\r\n",
    "    preprocessing_function=xception_pre_input,\r\n",
    "    rotation_range=0.1)\r\n",
    "\r\n",
    "train_generator = train_datagen.flow_from_directory(\r\n",
    "    \"raiox/train/\",\r\n",
    "    seed=seed_treino_validacao,\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(299,299))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#val_datagen = ImageDataGenerator(preprocessing_function=res_pre_input)\r\n",
    "\r\n",
    "val_generator = train_datagen.flow_from_directory(\r\n",
    "    \"raiox/train/\",\r\n",
    "    seed=seed_treino_validacao,\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(299,299),\r\n",
    "    subset=\"validation\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=xception_pre_input)\r\n",
    "\r\n",
    "test_generator = test_datagen.flow_from_directory(\r\n",
    "    \"raiox/test/\",\r\n",
    "    batch_size=32,\r\n",
    "    class_mode=\"categorical\",\r\n",
    "    color_mode=\"rgb\",\r\n",
    "    target_size=(299,299))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conv_base = Xception(include_top=False)\r\n",
    "\r\n",
    "for layer in conv_base.layers:\r\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = conv_base.output\r\n",
    "x = layers.GlobalAveragePooling2D()(x)\r\n",
    "x = layers.Dense(1024, activation='relu')(x)\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "\r\n",
    "predictions = layers.Dense(qtde_classes, activation='softmax')(x)\r\n",
    "model = Model(conv_base.input, predictions)\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = keras.optimizers.Adam()\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[precision_score, recall_score, f1_score])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpointer = ModelCheckpoint_tweaked(filepath='./modelo_xception.hdf5', verbose=1,  mode='max', save_best_only=True, monitor='val_f1_score')\r\n",
    "\r\n",
    "history = model.fit_generator(generator=train_generator, \r\n",
    "                              epochs=qtde_epocas, \r\n",
    "                              validation_steps=5, \r\n",
    "                              steps_per_epoch=5, \r\n",
    "                              validation_data=val_generator,\r\n",
    "                              callbacks=[checkpointer])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = load_model(\"./modelo_xception.hdf5\", custom_objects={'f1_score': f1_score, 'precision_score': precision_score, 'recall_score': recall_score})\r\n",
    "\r\n",
    "scores_xception = model.evaluate(test_generator)\r\n",
    "print()\r\n",
    "print(f\"Precision Score: {round(scores_xception[1]*100,2)}%\")\r\n",
    "print(f\"Recall Score: {round(scores_xception[2]*100,2)}%\")\r\n",
    "print(f\"F1 Score: {round(scores_xception[3]*100,2)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.8.1 Conclusões sobre a abordagem extra\n",
    "\n",
    "Como seu modelo performou em comparação com os demais modelos anteriores? Justifique sua resposta levando em consideração a arquitetura respectiva."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Resposta**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}